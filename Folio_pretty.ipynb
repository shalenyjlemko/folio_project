{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93edf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 10:36:40.805729: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_img, img_to_array\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load configuration data\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Please note: If running outside the provided Docker environment,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# update dataset_path in config.json to the local path where your dataset is stored.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Access values from configuration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Load configuration data\n",
    "# Please note: If running outside the provided Docker environment,\n",
    "# update dataset_path in config.json to the local path where your dataset is stored.\n",
    "\n",
    "with open('/workspace/folio_project/config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access values from configuration\n",
    "input_shape = tuple(config['input_shape'])\n",
    "target_size = tuple(config['target_size'])\n",
    "seed = config['seed']\n",
    "dataset_path = config['dataset_path']\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset path {dataset_path} does not exist. Please update dataset_path in config.json.\")\n",
    "    \n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    seed (int): The seed value.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "      titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8209b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data, a folder with 32 folders with photos for each species, it's \n",
    "ds_dir = '/dataset'\n",
    "\n",
    "# Lists to hold data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# Iterate through each folder (which represents a class) in the dataset directory\n",
    "for class_folder in os.listdir(ds_dir):\n",
    "    class_folder_path = os.path.join(ds_dir, class_folder)\n",
    "    if os.path.isdir(class_folder_path):\n",
    "          # Iterate through each image file in the folder\n",
    "        for img_file in os.listdir(class_folder_path):\n",
    "            img_file_path = os.path.join(class_folder_path, img_file)\n",
    "            # Load the image and convert it to an array\n",
    "            img = load_img(img_file_path, target_size=target_size)\n",
    "            img_array = img_to_array(img)\n",
    "            # Append the image array and label to the data and labels lists\n",
    "            data.append(img_array)\n",
    "            labels.append(class_folder)\n",
    "            \n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string labels to integers to avoid \"Cast string to float is not supported\"\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "# Checking if all is well\n",
    "print(f'Original unique labels: {len(np.unique(labels))}')\n",
    "print(f'Encoded unique labels: {len(np.unique(encoded_labels))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "def prepare_data(X_train, y_train, X_val, y_val):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    ])\n",
    "    \n",
    "    train_ds = (\n",
    "        train_ds\n",
    "        .cache()\n",
    "        .map(convert_to_float)\n",
    "        .map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "        .batch(128)  \n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .cache()\n",
    "        .map(convert_to_float)\n",
    "        .batch(128)  \n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def create_model(input_shape):\n",
    "    \"\"\"\n",
    "    Creates and compiles the model.\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): The shape of the input data.\n",
    "    \n",
    "    Returns:\n",
    "    model (tf.keras.Model): The compiled model.\n",
    "    \"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='softmax')\n",
    "    ])\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model_on_fold(model, train_ds, val_ds, epochs=20, batch_size=128):\n",
    "    history = model.fit(train_ds,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_ds) \n",
    "    return history.history\n",
    "\n",
    "\n",
    "def average_history(histories):\n",
    "    avg_history = {\n",
    "        'loss': np.mean([x['loss'] for x in histories], axis=0),\n",
    "        'val_loss': np.mean([x['val_loss'] for x in histories], axis=0),\n",
    "        'accuracy': np.mean([x['accuracy'] for x in histories], axis=0),\n",
    "        'val_accuracy': np.mean([x['val_accuracy'] for x in histories], axis=0)\n",
    "    }\n",
    "    return pd.DataFrame(avg_history)\n",
    "\n",
    "def plot_history(avg_history_frame):\n",
    "    avg_history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "    avg_history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()\n",
    "\n",
    "def perform_kfold_cross_validation(data, encoded_labels, n_splits=5, random_state=1):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    all_histories = []\n",
    "\n",
    "    for train_index, val_index in skf.split(data, encoded_labels):\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = encoded_labels[train_index], encoded_labels[val_index]\n",
    "\n",
    "        train_ds, val_ds = prepare_data(X_train, y_train, X_val, y_val)  \n",
    "\n",
    "        model = create_model(input_shape=input_shape)\n",
    "        history = train_model_on_fold(model, train_ds, val_ds)  \n",
    "        all_histories.append(history)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        cm = confusion_matrix(y_val, y_pred_classes)\n",
    "        unique_labels = np.unique(encoded_labels)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.show()\n",
    "\n",
    "        # Per-class Performance\n",
    "        print(classification_report(y_val, y_pred_classes))\n",
    "\n",
    "    avg_history_frame = average_history(all_histories)\n",
    "    plot_history(avg_history_frame)\n",
    "    \n",
    "    return avg_history_frame\n",
    "\n",
    "# Finally, call the function to perform K-fold cross validation:\n",
    "avg_history_frame = perform_kfold_cross_validation(data, encoded_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
